{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision-Language Model Experiments\n",
    "Integrating vision with robotic instructions is critical for more advanced embodied AI use cases. Many such cases involve cameras and sensors that can inspect or perceive the agent's environment, alongside context from language prompts. Here's an example scenario: `\"Hey, go check room 2441 and see if there's a red box in the corner near the lectern\"`. To feasibly execute this scenario, a machine learning model that can process the robot's stereo camera image will determine if a \"red box\" is present and if the robot is close to a lectern. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
