{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from inspect import signature\n",
    "from yaml import safe_load\n",
    "import pandas as pd \n",
    "import tqdm\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import faiss\n",
    "import gc\n",
    "from typing import Tuple\n",
    "import dotenv\n",
    "import time\n",
    "OUTPUT_DIR = Path(\"test_results\")\n",
    "today = pd.Timestamp.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRIFGenerator:\n",
    "    def __init__(self, \n",
    "                 prompt_template: str,\n",
    "                 waypoints_csv: str,\n",
    "                 language_model_type = \"Ollama\",\n",
    "                 embed_model_type = \"Ollama\",\n",
    "                 language_model: str = \"deepseek-r1:1.5b\",\n",
    "                 embed_model: str = \"deepseek-r1:1.5b\",\n",
    "                 chat_kwargs: dict = {\"num_predict\": 500, \"temperature\": 0.2, \"top_p\": 0.2},\n",
    "                 fetch_all: bool = True):\n",
    "        self.waypoints_list = []\n",
    "        self.kwargs = chat_kwargs\n",
    "        self.prompt_template = prompt_template\n",
    "        self.waypoints_csv = waypoints_csv\n",
    "        self.language_model_type = language_model_type\n",
    "        self.x_coord = 0\n",
    "        self.y_coord = 0\n",
    "        print(f\"Loading model: {language_model}...\")\n",
    "        \n",
    "        if embed_model_type == \"Ollama\":\n",
    "            embed = OllamaEmbeddings(model = embed_model)\n",
    "        elif embed_model_type == \"HuggingFace\":\n",
    "            embed = HuggingFaceEmbeddings(model_name = embed_model)\n",
    "        else:\n",
    "            raise Exception(\"Invalid embed_model_type\")\n",
    "\n",
    "        if language_model_type == \"Ollama\":\n",
    "            self.llm = OllamaLLM(model=language_model, **self.filter_kwargs_for_OllamaLLM(self.kwargs))\n",
    "        elif language_model_type == \"google\":\n",
    "            if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "                self.llm = GoogleGenerativeAI(model=language_model, **self.kwargs)\n",
    "            else:\n",
    "                raise Exception(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "        elif language_model_type == \"OpenAI\":\n",
    "            try: \n",
    "                self.llm = ChatOpenAI(model=language_model, **self.filter_kwargs_for_OpenAI(self.kwargs))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading OpenAI model: {e}\")\n",
    "        else:\n",
    "            raise Exception(\"Invalid language_model_type\")\n",
    "\n",
    "        self.output_parser = StrOutputParser()\n",
    "        waypoints = pd.read_csv(self.waypoints_csv)\n",
    "        waypoint_cols = ['Location', 'X co-ordinate', 'Y co-ordinate', 'Floor', 'Section', 'Keywords']\n",
    "        given_waypoints_cols = list(waypoints.columns)\n",
    "        if given_waypoints_cols != waypoint_cols:\n",
    "            raise Exception(\"Columns do not match, please provide a new dataset\")\n",
    "        \n",
    "        num_rows = len(waypoints)\n",
    "        print(f\"Processing {num_rows} waypoints...\")\n",
    "        with tqdm.tqdm(total=num_rows) as pbar:\n",
    "            for index, row in waypoints.iterrows():\n",
    "                self.waypoints_list.append(Document(id = f\"Waypoint_{index}\", page_content=f\"A waypoint called {row['Location']} exists at X-Coordinate, {row['X co-ordinate']}, and Y-Coordinate, {row['Y co-ordinate']}, apart of Section, {row['Section']}. Words associated with the waypoint, {row['Location']} are {row['Keywords']}.\"))\n",
    "                pbar.update(1)\n",
    "        \n",
    "        index = faiss.IndexFlatL2(len(embed.embed_query(\"Hello World!\")))\n",
    "\n",
    "        self.vector_store = FAISS(\n",
    "            embedding_function=embed,\n",
    "            index=index,\n",
    "            docstore=InMemoryDocstore(),\n",
    "            index_to_docstore_id={}            \n",
    "        )\n",
    "        \n",
    "        print(\"Adding waypoints to vector store...\")\n",
    "        self.vector_store.add_documents(self.waypoints_list)\n",
    "        print(\"Creating retriever...\")\n",
    "        k = len(self.waypoints_list) if fetch_all else 5\n",
    "        \n",
    "        self.retriever = self.vector_store.as_retriever(\n",
    "            search_type = \"mmr\",\n",
    "            search_kwargs = {'k': k, 'fetch_k': len(self.waypoints_list)},\n",
    "        )\n",
    "\n",
    "        with open(self.prompt_template, 'r') as file:\n",
    "            prompt_dict = safe_load(file)\n",
    "            self.prompt = prompt_dict['prompt']\n",
    "            self.prompt_name = prompt_dict['prompt_name']\n",
    "        \n",
    "        print(\"Creating prompt template...\")\n",
    "        self.prompt_template = PromptTemplate(template=self.prompt, input_variables=[\"documents\", \"query\", \"starting_location\"])\n",
    "\n",
    "        print(\"Creating RAG chain...\")\n",
    "        self.rag_chain = self.prompt_template | self.llm | self.output_parser\n",
    "        \n",
    "    def filter_kwargs_for_OllamaLLM(self, kwargs: dict) -> dict:\n",
    "        chatollama_params = signature(OllamaLLM.__init__).parameters\n",
    "        return {k: v for k, v in kwargs.items() if k in chatollama_params}\n",
    "    \n",
    "    def filter_kwargs_for_OpenAI(self, kwargs: dict) -> dict:\n",
    "        chatopenai_params = signature(ChatOpenAI.__init__).parameters\n",
    "        return {k: v for k, v in kwargs.items() if k in chatopenai_params}\n",
    "    \n",
    "    def generate_xrif_with_deepseek_model(self, query: str):\n",
    "        xrif_response = None\n",
    "        documents = self.retriever.invoke(query)\n",
    "        num_waypoints_injected = len(documents)\n",
    "        doc_texts = \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "        rendered_prompt = self.prompt_template.format(documents = doc_texts, query= query, starting_location = f\"X-Coordinate: {self.x_coord}, Y-Coordinate: {self.y_coord}\")\n",
    "\n",
    "        llm_response = self.rag_chain.invoke({\"documents\": doc_texts, \"query\": query, \"starting_location\": f\"X-Coordinate: {self.x_coord}, Y-Coordinate: {self.y_coord}\"})\n",
    "\n",
    "        xrif_response = re.sub(r\"<think>.*?</think>\", \"\", llm_response, flags=re.DOTALL)\n",
    "\n",
    "        xrif_response = re.sub(r\"\\n\", \"\", xrif_response)\n",
    "\n",
    "        xrif_response = re.sub(r\"\\\\n\", \"\", xrif_response)\n",
    "\n",
    "        start = xrif_response.find('{')\n",
    "        end = xrif_response.rfind('}')\n",
    "\n",
    "        if start != -1 and end != -1:\n",
    "            xrif_response = xrif_response[start:end+1]\n",
    "            try:\n",
    "                xrif_response = json.loads(xrif_response)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"Error parsing JSON response\")\n",
    "\n",
    "        \n",
    "        return xrif_response, llm_response, rendered_prompt, doc_texts, num_waypoints_injected\n",
    "    \n",
    "    def batch_test_run(self, df: pd.DataFrame, output_folder: Path):\n",
    "        num_rows = len(df)\n",
    "        print(f\"Processing {num_rows} test prompts...\")\n",
    "        with tqdm.tqdm(total=num_rows) as pbar:\n",
    "            for index, row in df.iterrows():\n",
    "                now = pd.Timestamp.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "                dump_json = {}\n",
    "                error_message = 'No Error'\n",
    "                note = ''\n",
    "                query = row['Prompt']\n",
    "                self.x_coord = row['Starting X']\n",
    "                self.y_coord = row['Starting Y']\n",
    "                expected_response = row['Expected Response']\n",
    "                llm_response, xrif_response = self.generate_xrif_with_deepseek_model(query)\n",
    "\n",
    "                dump_json['Data Set'] = row['Data Set']\n",
    "                dump_json['Experiment ID'] = row['Experiment ID']\n",
    "                dump_json['Prompt ID'] = row['Prompt ID']\n",
    "                dump_json['Prompt'] = query\n",
    "                dump_json['Full LLM Response'] = str(llm_response)\n",
    "                dump_json['XRIF Generated'] = xrif_response if isinstance(xrif_response, dict) else str(xrif_response)\n",
    "                dump_json['Starting X'] = self.x_coord\n",
    "                dump_json['Starting Y'] = self.y_coord\n",
    "                dump_json['Expected Response'] = expected_response\n",
    "                dump_json['Expected Response Type'] = row['Expected Response Type']\n",
    "                expected_response = list(expected_response)\n",
    "                if xrif_response and type(xrif_response) == dict:\n",
    "                    if 'actions' in xrif_response.keys():\n",
    "                        actions = xrif_response['actions']\n",
    "                        if len(expected_response) == len(actions):\n",
    "                            if row['Expected Response Type'] == 'Nav':\n",
    "                                list_of_locations = [action['input']['name'] for action in actions if (action['action'] == 'navigate') and ('input' in action) and ('name' in action['input'])]\n",
    "                                set_of_locations = set(list_of_locations)\n",
    "                                set_of_expected_locations = set(expected_response)\n",
    "                                if set_of_locations == set_of_expected_locations:\n",
    "                                    note = \"All locations are present in the response\"\n",
    "                                    dump_json['Notes'] = note\n",
    "                            for i in range(len(xrif_response['actions'])):\n",
    "                                if actions[i]['action'] == 'navigate' :\n",
    "                                    if type(expected_response[i]) == 'str':\n",
    "                                        if 'input' in actions[i]:\n",
    "                                            if 'name' in actions[i]['input'].keys():\n",
    "                                                if actions[i]['input']['name'] == expected_response[i] or (type(expected_response[i]) == 'list' and actions[i]['input']['name'] in expected_response[i]):\n",
    "                                                    continue\n",
    "                                                else:\n",
    "                                                    error_message = f\"Expected location name: {actions[i]['name']} does not match with the provided location name: {expected_response[i]}\"\n",
    "                                                    dump_json['Error Message'] = error_message\n",
    "                                                    break\n",
    "                                            else:\n",
    "                                                error_message = f\"Response Error Missing field: 'name' in action object {i}\"\n",
    "                                                dump_json['Error Message'] = error_message\n",
    "                                                break\n",
    "                                        else:\n",
    "                                            error_message = f\"Response Error Missing field: 'input' in action object {i}\"\n",
    "                                            dump_json['Error Message'] = error_message\n",
    "                                            break\n",
    "                                    else:\n",
    "                                        error_message = f\"Expected Response Error: Expected Response Type is not 'Nav' for action object {i}\"\n",
    "                                        dump_json['Error Message'] = error_message\n",
    "                                        break\n",
    "                                elif actions[i]['action'] == 'wait':\n",
    "                                    if type(expected_response[i]) == 'tuple':\n",
    "                                        if 'input' in actions[i]:\n",
    "                                                if actions[i]['action'] != expected_response[i][0]:\n",
    "                                                    error_message = f\"Expected action at action object {i} : {expected_response[i][0]}  does not match with the provided action: {actions[i]['action']}\"\n",
    "                                                    dump_json['Error Message'] = error_message\n",
    "                                                    break\n",
    "                                                else:\n",
    "                                                    if (int(actions[i]['input']))/60 == expected_response[i][1]:\n",
    "                                                        error_message = f\"Expected wait time: {actions[i]['input']} does not match with the provided wait time: {expected_response[i][1]}\"\n",
    "                                                        dump_json['Error Message'] = error_message\n",
    "                                                        break\n",
    "                                        else:\n",
    "                                            error_message = f\"Response Error Missing field: 'input' in action object {i}\"\n",
    "                                            dump_json['Error Message'] = error_message\n",
    "                                            break\n",
    "                                    else:\n",
    "                                        error_message = f\"Expected Response Error: Expected Response Type is not 'wait' for action object {i}\"\n",
    "                                        dump_json['Error Message'] = error_message\n",
    "                                        break\n",
    "                                elif actions[i]['action'] == 'speak':\n",
    "                                    if type(expected_response[i]) == 'tuple':\n",
    "                                        if ('action' in actions[i]) and ('input' in actions[i]):\n",
    "                                            if actions[i]['action'] == expected_response[i][0] and (actions[i]['input'] == expected_response[i][1] or (type(expected_response[i]) == 'list' and actions[i]['input'] in expected_response[i])):\n",
    "                                                continue\n",
    "                                            else:\n",
    "                                                error_message = f\"Expected speak message: {expected_response[i]} does not match with the generated speak message: {actions[i]['input']} or Expected action: {expected_response[i][0]} does not match with the generated action: {actions[i]['action']}\"\n",
    "                                                dump_json['Error Message'] = error_message\n",
    "                                                break\n",
    "                                        else:\n",
    "                                            error_message = f\"Response Error Missing field: 'input' or 'action' in action object {i}\"\n",
    "                                            dump_json['Error Message'] = error_message\n",
    "                                            break\n",
    "                                    else:\n",
    "                                        error_message = f\"Expected Response Error: Expected Response Type is not correct for action object {i}\"\n",
    "                                        dump_json['Error Message'] = error_message\n",
    "                                        break\n",
    "                else:\n",
    "                    error_message = \"XRIF response is Invalid\"\n",
    "                    dump_json['Error Message'] = error_message\n",
    "                \n",
    "                if not os.path.exists(OUTPUT_DIR / output_folder):\n",
    "                    os.makedirs(OUTPUT_DIR / output_folder)\n",
    "\n",
    "                with open(OUTPUT_DIR / output_folder / f\"Prompt_{row['Prompt ID']}_{now}.json\", 'w') as file:\n",
    "                    json.dump(dump_json, file, indent=4)\n",
    "                pbar.update(1)\n",
    "\n",
    "        return OUTPUT_DIR / output_folder\n",
    "\n",
    "    def generate_xrif_with_google_model(self, query: str):\n",
    "        xrif_response = None\n",
    "        documents = self.retriever.invoke(query)\n",
    "        num_waypoints_injected = len(documents) \n",
    "        doc_texts = \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "        rendered_prompt = self.prompt_template.format(documents=doc_texts, query = query, starting_location = f\"X-Coordinate: {self.x_coord}, Y-Coordinate: {self.y_coord}\")\n",
    "\n",
    "        llm_response = self.rag_chain.invoke({\"documents\": doc_texts, \"query\": query, \"starting_location\": f\"X-Coordinate: {self.x_coord}, Y-Coordinate: {self.y_coord}\"})\n",
    "\n",
    "        try:\n",
    "            xrif_response = llm_response.strip(\"```json\\n```\")\n",
    "            xrif_response = json.loads(xrif_response)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON response: {e}\")\n",
    "            xrif_response = llm_response\n",
    "\n",
    "        return xrif_response, llm_response, rendered_prompt, doc_texts, num_waypoints_injected\n",
    "\n",
    "    def eval_xrif(self, *, eval_json: dict, xrif_gen: dict, xrif_ex: list, response_type):\n",
    "        error_message = None\n",
    "        dump_json = eval_json\n",
    "        xrif_ex = list(xrif_ex)\n",
    "        if xrif_gen and type(xrif_gen) == dict:\n",
    "            if 'actions' in xrif_gen.keys():\n",
    "                actions = xrif_gen['actions']\n",
    "                if len(xrif_ex) == len(actions):\n",
    "                    if response_type == 'Nav':\n",
    "                        list_of_locations = [action['input']['name'] for action in actions if ('action' in action and action['action'] == 'navigate') and ('input' in action) and ('name' in action['input'])]\n",
    "                        set_of_locations = set(list_of_locations)\n",
    "                        set_of_expected_locations = set(xrif_ex)\n",
    "                        if set_of_locations == set_of_expected_locations:\n",
    "                            note = \"All locations are present in the response\"\n",
    "                            dump_json['Notes'] = note\n",
    "                    for i in range(len(xrif_gen['actions'])):\n",
    "                        if 'action' in actions[i]:\n",
    "                            if actions[i]['action'] == 'navigate' :\n",
    "                                if type(xrif_ex[i]) is str:\n",
    "                                    if 'input' in actions[i]:\n",
    "                                        if 'name' in actions[i]['input'].keys():\n",
    "                                            if actions[i]['input']['name'] == xrif_ex[i] or (type(xrif_ex[i]) == 'list' and actions[i]['input']['name'] in xrif_ex[i]):\n",
    "                                                continue\n",
    "                                            else:\n",
    "                                                error_message = f\"Expected location name: {actions[i]['input']['name']} does not match with the provided location name: {xrif_ex[i]}\"\n",
    "                                                dump_json['Error Message'] = error_message\n",
    "                                                break\n",
    "                                        else:\n",
    "                                            error_message = f\"Response Error Missing field: 'name' in action object {i}\"\n",
    "                                            dump_json['Error Message'] = error_message\n",
    "                                            break\n",
    "                                    else:\n",
    "                                        error_message = f\"Response Error Missing field: 'input' in action object {i}\"\n",
    "                                        dump_json['Error Message'] = error_message\n",
    "                                        break\n",
    "                                else:\n",
    "                                    error_message = f\"Expected Response Error: Expected Response Type is not 'Nav' for action object {i}\"\n",
    "                                    dump_json['Error Message'] = error_message\n",
    "                                    break\n",
    "                            elif actions[i]['action'] == 'wait':\n",
    "                                if type(xrif_ex[i]) is tuple:\n",
    "                                    if 'input' in actions[i]:\n",
    "                                            if actions[i]['action'] != xrif_ex[i][0]:\n",
    "                                                error_message = f\"Expected action at action object {i} : {xrif_ex[i][0]}  does not match with the provided action: {actions[i]['action']}\"\n",
    "                                                dump_json['Error Message'] = error_message\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                if (int(actions[i]['input']))/60 == xrif_ex[i][1]:\n",
    "                                                    error_message = f\"Expected wait time: {actions[i]['input']} does not match with the provided wait time: {xrif_ex[i][1]}\"\n",
    "                                                    dump_json['Error Message'] = error_message\n",
    "                                                    break\n",
    "                                    else:\n",
    "                                        error_message = f\"Response Error Missing field: 'input' in action object {i}\"\n",
    "                                        dump_json['Error Message'] = error_message\n",
    "                                        break\n",
    "                                else:\n",
    "                                    error_message = f\"Expected Response Error: Expected Response Type is not 'wait' for action object {i}\"\n",
    "                                    dump_json['Error Message'] = error_message\n",
    "                                    break\n",
    "                            elif actions[i]['action'] == 'speak':\n",
    "                                if type(xrif_ex[i]) is tuple:\n",
    "                                    if ('action' in actions[i]) and ('input' in actions[i]):\n",
    "                                        if actions[i]['action'] == xrif_ex[i][0] and (actions[i]['input'] == xrif_ex[i][1] or (type(xrif_ex[i]) == 'list' and actions[i]['input'] in xrif_ex[i])):\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                            error_message = f\"Expected speak message: {xrif_ex[i]} does not match with the generated speak message: {actions[i]['input']} or Expected action: {xrif_ex[i][0]} does not match with the generated action: {actions[i]['action']}\"\n",
    "                                            dump_json['Error Message'] = error_message\n",
    "                                            break\n",
    "                                    else:\n",
    "                                        error_message = f\"Response Error Missing field: 'input' or 'action' in action object {i}\"\n",
    "                                        dump_json['Error Message'] = error_message\n",
    "                                        break\n",
    "                                else:\n",
    "                                    error_message = f\"Expected Response Error: Expected Response Type is not correct for action object {i}\"\n",
    "                                    dump_json['Error Message'] = error_message\n",
    "                                    break\n",
    "                        else:\n",
    "                            error_message = f\"Response Error Missing field: 'action' in action object {i}\"\n",
    "                            dump_json['Error Message'] = error_message\n",
    "                            break\n",
    "        else:\n",
    "            error_message = \"XRIF response is Invalid\"\n",
    "            dump_json['Error Message'] = error_message\n",
    "        if not error_message:\n",
    "            dump_json['Error Message'] = \"\"\n",
    "        return dump_json\n",
    "    \n",
    "    def generate_xrif_with_openai_model(self, query: str):\n",
    "        xrif_response = None\n",
    "        documents = self.retriever.invoke(query)\n",
    "        num_waypoints_injected = len(documents)\n",
    "        doc_texts = \"\\n\".join([doc.page_content for doc in documents])\n",
    "        rendered_prompt = self.prompt_template.format(documents=doc_texts, query = query, starting_location = f\"X-Coordinate: {self.x_coord}, Y-Coordinate: {self.y_coord}\")\n",
    "\n",
    "        llm_response = self.rag_chain.invoke({\"documents\": doc_texts, \"query\": query, \"starting_location\": f\"X-Coordinate: {self.x_coord}, Y-Coordinate: {self.y_coord}\"})\n",
    "\n",
    "        try:\n",
    "            xrif_response = llm_response.strip(\"```json\\n```\")\n",
    "            xrif_response = json.loads(xrif_response)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON response: {e}\")\n",
    "            xrif_response = llm_response\n",
    "\n",
    "        return xrif_response, llm_response, rendered_prompt, doc_texts, num_waypoints_injected\n",
    "\n",
    "    def xrif_inference(self, query: str):\n",
    "        if self.language_model_type == \"Ollama\":\n",
    "            return self.generate_xrif_with_deepseek_model(query)\n",
    "        elif self.language_model_type == \"google\":\n",
    "            return self.generate_xrif_with_google_model(query)\n",
    "        elif self.language_model_type == \"OpenAI\":\n",
    "            return self.generate_xrif_with_openai_model(query)\n",
    "        else:\n",
    "            raise Exception(\"Invalid model type\")\n",
    "        \n",
    "    def batch_experiment_run(self, df: pd.DataFrame, output_folder: Path, sleep_time: int = 0):\n",
    "        num_rows = len(df)\n",
    "        print(f\"Processing {num_rows} test prompts...\")\n",
    "        with tqdm.tqdm(total=num_rows) as pbar:\n",
    "            for index, row in df.iterrows():\n",
    "                now = pd.Timestamp.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "                prompt_result = {}\n",
    "                query = row['Prompt']\n",
    "                self.x_coord = row['Starting X']\n",
    "                self.y_coord = row['Starting Y']\n",
    "                expected_response = row['Expected Response']\n",
    "                xrif_response, llm_response, rendered_prompt, doc_texts, num_waypoints_injected = self.xrif_inference(query)\n",
    "                prompt_result['rendered_prompt'] = rendered_prompt\n",
    "                prompt_result['Waypoints Injected'] = doc_texts\n",
    "                prompt_result['num_waypoints_injected'] = num_waypoints_injected\n",
    "                prompt_result['Data Set'] = row['Data Set']\n",
    "                prompt_result['Experiment ID'] = row['Experiment ID']\n",
    "                prompt_result['Prompt ID'] = row['Prompt ID']\n",
    "                prompt_result['Prompt'] = query\n",
    "                prompt_result['Full LLM Response'] = str(llm_response)\n",
    "                prompt_result['XRIF Generated'] = xrif_response if isinstance(xrif_response, dict) else str(xrif_response)\n",
    "                prompt_result['Starting X'] = self.x_coord\n",
    "                prompt_result['Starting Y'] = self.y_coord\n",
    "                prompt_result['Expected Response'] = expected_response\n",
    "                prompt_result['Expected Response Type'] = row['Expected Response Type']\n",
    "                expected_response = list(expected_response)\n",
    "                dump_json = self.eval_xrif(eval_json = prompt_result, xrif_gen = xrif_response, xrif_ex = expected_response, response_type = row['Expected Response Type'])\n",
    "                if not os.path.exists(OUTPUT_DIR / output_folder):\n",
    "                    os.makedirs(OUTPUT_DIR / output_folder)\n",
    "\n",
    "                with open(OUTPUT_DIR / output_folder / f\"Prompt_{row['Prompt ID']}_{now}.json\", 'w') as file:\n",
    "                    json.dump(dump_json, file, indent=4)\n",
    "\n",
    "                if sleep_time > 0:\n",
    "                    time.sleep(sleep_time)\n",
    "                pbar.update(1)\n",
    "\n",
    "        return OUTPUT_DIR / output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_dataset(dataset: pd.DataFrame):\n",
    "    dataset['Expected Response'] = dataset['Expected Response'].to_list()\n",
    "    dataset.dropna(subset=['Prompt', 'Expected Response'], inplace=True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_experiment_from_config(config_file: Path) -> Tuple[XRIFGenerator, pd.DataFrame, Path]:\n",
    "    with open(config_file, 'r') as file:\n",
    "        config_dict = safe_load(file)\n",
    "        test_dataset_path = config_dict['test_dataset']\n",
    "\n",
    "        test_dataset = pd.read_csv(test_dataset_path)\n",
    "        output_folder = config_dict['output_folder']\n",
    "\n",
    "        del config_dict['output_folder']\n",
    "        del config_dict['test_dataset']\n",
    "\n",
    "        test_dataset = process_test_dataset(test_dataset)\n",
    "        \n",
    "        return XRIFGenerator(**config_dict), test_dataset, output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: gemini-2.0-flash...\n",
      "Processing 20 waypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 37516.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding waypoints to vector store...\n",
      "Creating retriever...\n",
      "Creating prompt template...\n",
      "Creating RAG chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "google_xrif = XRIFGenerator(prompt_template=\"prompts/e7_xrif_actions_6.yaml\", \n",
    "                            waypoints_csv=\"waypoint_datasets/uw_e7_floor1.csv\",\n",
    "                            language_model_type=\"google\",\n",
    "                            embed_model_type=\"HuggingFace\",\n",
    "                            language_model=\"gemini-2.0-flash\",\n",
    "                            embed_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                            chat_kwargs={\"max_tokens\": 500, \"temperature\": 0.2, \"top_p\": 0.2},\n",
    "                            fetch_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: gpt-4o-mini...\n",
      "Processing 20 waypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 38746.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding waypoints to vector store...\n",
      "Creating retriever...\n",
      "Creating prompt template...\n",
      "Creating RAG chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "openai_xrif = XRIFGenerator(prompt_template=\"prompts/e7_xrif_actions_6.yaml\",\n",
    "                            waypoints_csv=\"waypoint_datasets/uw_e7_floor1.csv\",\n",
    "                            language_model_type=\"OpenAI\",\n",
    "                            embed_model_type=\"HuggingFace\",\n",
    "                            language_model=\"gpt-4o-mini\",\n",
    "                            embed_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                            chat_kwargs={\"num_predict\": 500, \"temperature\": 0.2, \"top_p\": 0.2},\n",
    "                            fetch_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: gpt-4o-mini...\n",
      "Processing 34 waypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 35821.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding waypoints to vector store...\n",
      "Creating retriever...\n",
      "Creating prompt template...\n",
      "Creating RAG chain...\n",
      "Processing 50 test prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:11<00:00,  8.63s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('test_results/gpt4o_mini_e5_hgf_embed')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_generator, experiment_dataframe, output_folder = load_experiment_from_config(Path('experiment_configs/openai/gpt4o_mini_e5_hgf_embed.yaml'))\n",
    "\n",
    "experiment_generator.batch_experiment_run(experiment_dataframe, output_folder, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: gemini-2.0-flash...\n",
      "Processing 24 waypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 34580.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding waypoints to vector store...\n",
      "Creating retriever...\n",
      "Creating prompt template...\n",
      "Creating RAG chain...\n",
      "Processing 60 test prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [11:24<00:00, 11.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: gemini-2.0-flash...\n",
      "Processing 34 waypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 34272.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding waypoints to vector store...\n",
      "Creating retriever...\n",
      "Creating prompt template...\n",
      "Creating RAG chain...\n",
      "Processing 50 test prompts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [10:18<00:00, 12.36s/it]\n"
     ]
    }
   ],
   "source": [
    "config_dir = 'experiment_configs/google'\n",
    "for config_file in os.listdir(config_dir):\n",
    "    experiment_generator, experiment_dataframe, output_folder = load_experiment_from_config(Path(config_dir) / config_file)\n",
    "    experiment_generator.batch_experiment_run(experiment_dataframe, output_folder, 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for eval_test...\n",
      "Results for gemini_warehouse_hgf_embed...\n",
      "Results for all_warehouse_deepseek1.5b...\n",
      "Results for gpt4o_mini_warehouse_hgf_embed...\n",
      "Results for eval_test.csv...\n",
      "Results for gemini_e7_hgf_embed.csv...\n",
      "Results for all_warehouse_deepseek14b...\n",
      "Results for all_e7_deepseek7b...\n",
      "Results for gpt4o_mini_e7_hgf_embed.csv...\n",
      "Results for all_e7_deepseek7b_hfembed.csv...\n",
      "Results for gpt4o_mini_e5_hgf_embed...\n",
      "Results for all_e7_deepseek1.5b.csv...\n",
      "Results for all_e7_deepseek14b.csv...\n",
      "Results for all_e7_deepseek14b...\n",
      "Results for gemini_e7_hgf_embed...\n",
      "Results for all_e7_deepseek1.5b...\n",
      "Results for all_e7_deepseek7b.csv...\n",
      "Results for all_e5_deepseek14b.yaml...\n",
      "Results for gpt4o_mini_warehouse_hgf_embed.csv...\n",
      "Results for all_warehouse_deepseek14b.csv...\n",
      "Results for gemini_warehouse_hgf_embed.csv...\n",
      "Results for all_warehouse_deepseek7b...\n",
      "Results for gpt4o_mini_e7_hgf_embed...\n",
      "Results for gpt4o_mini_e5_hgf_embed.csv...\n",
      "Results for gemini_e5_hgf_embed...\n",
      "Results for all_e7_deepseek7b_hfembed...\n",
      "Results for gemini_e5_hgf_embed.csv...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "for folder in os.listdir('test_results'):\n",
    "    print(f\"Results for {folder}...\")\n",
    "    if Path('test_results/' + folder).is_dir():\n",
    "        json_list = []\n",
    "        for file in os.listdir('test_results/' + folder):\n",
    "            with open(Path('test_results/' + folder) / file, 'r') as f:\n",
    "                json_list.append(json.load(f))\n",
    "        \n",
    "        df = pd.DataFrame(json_list)\n",
    "        df.to_csv(f\"test_results/{folder}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
