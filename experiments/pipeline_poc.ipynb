{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline Proof-of-Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from typing import Dict, Any\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.onnx\n",
    "from torch.quantization import quantize_dynamic\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import ray\n",
    "from ray import train\n",
    "import wandb\n",
    "import onnx\n",
    "import horovod.torch as hvd\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# NOTE: This cell is tagger `parameters`.\n",
    "model_name = \"gpt2\"  # @param\n",
    "data_path = \"data/train.txt\"  # @param\n",
    "output_dir = \"tmp\"  # @param\n",
    "wandb_project = \"llm-finetuning\"  # @param\n",
    "wandb_run_name = \"test-run\"  # @param\n",
    "distributed = False  # @param\n",
    "batch_size = 8  # @param\n",
    "num_workers = 4  # @param\n",
    "max_length = 512  # @param\n",
    "learning_rate = 5e-5  # @param\n",
    "weight_decay = 0.01  # @param\n",
    "num_epochs = 3  # @param\n",
    "warmup_steps = 500  # @param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): # NOTE: Apple NPU\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if distributed:\n",
    "    ray.init()\n",
    "    hvd.init()\n",
    "    torch.cuda.set_device(hvd.local_rank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data_path: str, tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        with open(data_path, 'r') as f:\n",
    "            self.texts = f.readlines()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not distributed or hvd.rank() == 0:\n",
    "    api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "    if api_key:\n",
    "        wandb.login()\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run_name,\n",
    "            config={\n",
    "                'model_name': model_name,\n",
    "                'batch_size': batch_size,\n",
    "                'learning_rate': learning_rate,\n",
    "                'num_epochs': num_epochs,\n",
    "                'max_length': max_length\n",
    "            }\n",
    "        )\n",
    "        print(\"Weights and Biases initialized successfully.\")\n",
    "    else:\n",
    "        print(\"Error: WANDB_API_KEY environment variable is not set. Please define it before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
